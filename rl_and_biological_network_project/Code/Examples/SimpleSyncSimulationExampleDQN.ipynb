{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8896b65",
   "metadata": {},
   "source": [
    "# Testing for DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1f0769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Add parent directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "current_dir = Path().resolve()\n",
    "root_dir = current_dir.parent\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.insert(0,str(root_dir))\n",
    "\n",
    "from Gyms.SimulatedNetworkSync import SimulatedNetworkSync\n",
    "from Gyms.RealNetworkSync import RealNetworkSync\n",
    "from Algorithms.DQN import DQN, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e940450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Discretize MultiDiscrete Actions ----\n",
    "def index_to_action(index, action_dim, action_n):\n",
    "    # Convert a flat index into MultiDiscrete action\n",
    "    action = []\n",
    "    for _ in range(action_dim):\n",
    "        action.append(index % action_n)\n",
    "        index //= action_n\n",
    "    return np.array(list(reversed(action)))\n",
    "\n",
    "def action_to_index(action, action_n):\n",
    "    index = 0\n",
    "    for a in action:\n",
    "        index = index * action_n + a\n",
    "    return index\n",
    "\n",
    "def select_action(env, state, policy_net, epsilon, action_dim, action_n):\n",
    "    # Epsilon-greedy action\n",
    "    if random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "        action_idx = action_to_index(action, action_n)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            action_idx = torch.argmax(q_values).item()\n",
    "            action = index_to_action(action_idx, action_dim, action_n)\n",
    "    \n",
    "    return action, action_idx\n",
    "\n",
    "# ---- Train Loop ----\n",
    "def train(env, episodes=200, steps_per_episode=100, batch_size=64, gamma=0.99, \n",
    "          epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "          buffer_capacity=10000, lr=1e-3, target_update_freq=10):\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.nvec.shape[0]\n",
    "    action_n = env.action_space.nvec[0]\n",
    "\n",
    "    policy_net = DQN(state_dim, action_dim, action_n)\n",
    "    target_net = DQN(state_dim, action_dim, action_n)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(steps_per_episode):\n",
    "            action, action_idx = select_action(env, state, policy_net, epsilon, action_dim, action_n)\n",
    "\n",
    "            next_state, reward, _, _, _ = env.step(action)\n",
    "            replay_buffer.push(state, action_idx, reward, next_state, False)  # done=False always\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Train step\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                states = torch.FloatTensor(states)\n",
    "                actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "                rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "                next_states = torch.FloatTensor(next_states)\n",
    "                dones = torch.BoolTensor(dones).unsqueeze(1)\n",
    "\n",
    "                q_values = policy_net(states).gather(1, actions)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "                    target_q = rewards + gamma * next_q_values * (~dones)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # Periodically update the target network\n",
    "        if episode % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # print(f\"Episode {episode} | Total Reward: {total_reward:.2f} | Epsilon: {epsilon:.3f}\")\n",
    "        print(f\"Episode {episode} | Current Reward: {reward:.2f} | Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    return policy_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47215661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, model, episodes=10, steps_per_episode=100, render=False):\n",
    "    model.eval()\n",
    "\n",
    "    action_dim = env.action_space.nvec.shape[0]\n",
    "    action_n = env.action_space.nvec[0]\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(steps_per_episode):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_tensor)\n",
    "                action_idx = torch.argmax(q_values).item()\n",
    "                action = index_to_action(action_idx, action_dim, action_n)\n",
    "\n",
    "            state, reward, _, _, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        print(f\"Test Episode {episode} | Total Reward: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edecdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host/Port open and accessable\n",
      "Episode 0 | Current Reward: 1.00 | Epsilon: 0.990\n",
      "Episode 1 | Current Reward: -1.00 | Epsilon: 0.980\n",
      "Episode 2 | Current Reward: 3.00 | Epsilon: 0.970\n",
      "Episode 3 | Current Reward: 1.00 | Epsilon: 0.961\n",
      "Episode 4 | Current Reward: 1.00 | Epsilon: 0.951\n",
      "Episode 5 | Current Reward: 1.00 | Epsilon: 0.941\n",
      "Episode 6 | Current Reward: 2.00 | Epsilon: 0.932\n",
      "Episode 7 | Current Reward: -2.00 | Epsilon: 0.923\n",
      "Episode 8 | Current Reward: 0.00 | Epsilon: 0.914\n",
      "Episode 9 | Current Reward: 1.00 | Epsilon: 0.904\n",
      "Episode 10 | Current Reward: -2.00 | Epsilon: 0.895\n",
      "Episode 11 | Current Reward: 1.00 | Epsilon: 0.886\n",
      "Episode 12 | Current Reward: 1.00 | Epsilon: 0.878\n",
      "Episode 13 | Current Reward: -1.00 | Epsilon: 0.869\n",
      "Episode 14 | Current Reward: 0.00 | Epsilon: 0.860\n",
      "Episode 15 | Current Reward: 4.00 | Epsilon: 0.851\n",
      "Episode 16 | Current Reward: 2.00 | Epsilon: 0.843\n",
      "Episode 17 | Current Reward: -1.00 | Epsilon: 0.835\n",
      "Episode 18 | Current Reward: 0.00 | Epsilon: 0.826\n",
      "Episode 19 | Current Reward: 3.00 | Epsilon: 0.818\n",
      "Episode 20 | Current Reward: -2.00 | Epsilon: 0.810\n",
      "Episode 21 | Current Reward: -2.00 | Epsilon: 0.802\n",
      "Episode 22 | Current Reward: 4.00 | Epsilon: 0.794\n",
      "Episode 23 | Current Reward: -2.00 | Epsilon: 0.786\n",
      "Episode 24 | Current Reward: -1.00 | Epsilon: 0.778\n",
      "Episode 25 | Current Reward: -1.00 | Epsilon: 0.770\n",
      "Episode 26 | Current Reward: -1.00 | Epsilon: 0.762\n",
      "Episode 27 | Current Reward: 4.00 | Epsilon: 0.755\n",
      "Episode 28 | Current Reward: 3.00 | Epsilon: 0.747\n",
      "Episode 29 | Current Reward: 2.00 | Epsilon: 0.740\n",
      "Episode 30 | Current Reward: 1.00 | Epsilon: 0.732\n",
      "Episode 31 | Current Reward: 0.00 | Epsilon: 0.725\n",
      "Episode 32 | Current Reward: 4.00 | Epsilon: 0.718\n",
      "Episode 33 | Current Reward: 0.00 | Epsilon: 0.711\n",
      "Episode 34 | Current Reward: 2.00 | Epsilon: 0.703\n",
      "Episode 35 | Current Reward: -2.00 | Epsilon: 0.696\n",
      "Episode 36 | Current Reward: -1.00 | Epsilon: 0.689\n",
      "Episode 37 | Current Reward: 4.00 | Epsilon: 0.683\n",
      "Episode 38 | Current Reward: 0.00 | Epsilon: 0.676\n",
      "Episode 39 | Current Reward: 1.00 | Epsilon: 0.669\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment parameters\n",
    "action_dim = 5 # Number of dimensions in each action (5 time steps)\n",
    "state_dim = 10 # Number of features in the state representation\n",
    "\n",
    "# env = SimulatedNetworkSync(action_dim=action_dim, state_dim=state_dim)\n",
    "env = RealNetworkSync(action_dim=action_dim, state_dim=state_dim, circuit_id=3)\n",
    "trained_model = train(env, episodes=40, steps_per_episode=20, epsilon_decay=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6966c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 0 | Total Reward: 46.00\n",
      "Test Episode 1 | Total Reward: 41.00\n",
      "Test Episode 2 | Total Reward: 37.00\n",
      "Test Episode 3 | Total Reward: 22.00\n",
      "Test Episode 4 | Total Reward: 34.00\n"
     ]
    }
   ],
   "source": [
    "test_policy(env, trained_model, episodes=5, steps_per_episode=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
