{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2548fafd",
   "metadata": {},
   "source": [
    "# TESTING WITH MULTI ARMED BANDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7029876-da98-42b6-a393-2317db517ac6",
   "metadata": {},
   "source": [
    "This jupyter gives you a simple example of how you should use the Simulated Network (asynchronous) environment. This environment is not meant as a training ground of your algorithms, but only to check whether or not your algorithm can be executed and if it is fast enough to create a response in time. (Set stim_period = 100 to be sure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92edcf65-93c1-4396-90d0-4572ec09c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Add parent directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "current_dir = Path().resolve()\n",
    "root_dir = current_dir.parent\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.insert(0,str(root_dir))\n",
    "\n",
    "from Gyms.SimulatedNetworkSync import SimulatedNetworkSync\n",
    "\n",
    "from Reward.TrainingReward import TrainingReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a528d5-af3f-4c5b-9bef-0fe0022a21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define size of state and action spaces, as well as stimulation period\n",
    "state_dim   = 4   # Dimension of reduced state space\n",
    "action_dim  = 5   # Number of stimuli in action space. Must be less or equal to 5 (each stimulus needs a value of {0,1,2,3,4}\n",
    "stim_period = 250 # stimulation period, meaning that an action is applied every {stim_period} ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5373dba-d31c-4f31-9906-6db506053bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and initialize it\n",
    "env      = SimulatedNetworkSync(action_dim=action_dim,state_dim=state_dim,stim_period=stim_period)\n",
    "state, _ = env.reset()\n",
    "env.render() # This function gives you the current state + reward, which both is 0 after initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Algorithms.MultiArmedBandit import MABAgent  # Update path if needed\n",
    "\n",
    "# Initialize MAB agent\n",
    "agent = MABAgent(\n",
    "    epsilon=0.9,            # Initial exploration rate (90% random actions)\n",
    "    alpha=0.1,              # Learning rate (constant step-size)\n",
    "    initial_q=0.0,          # Optimistic initial values\n",
    "    n_actions=25,           # Number of actions, should be a power of 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4cc22-d672-490f-93c6-d98b3ca82de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example code, that stimulates the network 10_000 times with a randomly sampled action, while calculating also the average reward received\n",
    "\n",
    "total_reward = 0\n",
    "action_count = 0\n",
    "\n",
    "rewards_over_time = []\n",
    "\n",
    "for _ in range(10_000):\n",
    "    # For simplicity, choose a random action\n",
    "    action_idx = agent.select_action()\n",
    "    action = agent.action_map(action_idx)\n",
    "    # print(f\"Stimulate with action: {action}\")\n",
    "    \n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    action_count += 1\n",
    "\n",
    "    rewards_over_time.append(reward)\n",
    "\n",
    "    agent.update(action_idx, reward)\n",
    "\n",
    "    # Plot information\n",
    "    # print(f\"Info: {info}, reward: {reward}\")\n",
    "\n",
    "    # print(\"-----------------------------\")\n",
    "\n",
    "    if _ % 100 == 0:\n",
    "        print(f\"Step: {_}, Action: {action}, Reward: {reward}, Total reward: {total_reward}, Average reward: {total_reward/action_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards_over_time)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01d04d-21a2-4760-9f81-7bfe2884a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average reward: {total_reward/action_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
