{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f936f3-1e74-4024-8ffd-dc3216e8cb22",
   "metadata": {},
   "source": [
    "This jupyter notebook teaches you how to interact with the real neuronal networks (static state function).\n",
    "\n",
    "Before you run this code, set your group_id and password in \"auth.py\". Ask your mentor for your password/id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92edcf65-93c1-4396-90d0-4572ec09c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "current_dir = Path().resolve()\n",
    "root_dir = current_dir.parent\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.insert(0,str(root_dir))\n",
    "\n",
    "from Gyms.RealNetworkSync import RealNetworkSync\n",
    "from Algorithms.MultiArmedBandit import MABAgent, MAB_STRATEGIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a936a1e1-9664-4e4a-b123-bca7d7243b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define size of state and action spaces, as well as stimulation period\n",
    "state_dim   = 4   # Dimension of reduced state space\n",
    "action_dim  = 5   # Number of stimuli in action space\n",
    "circuit_id  = 2   # Each group has 4 biological/simulated circuits. You choose here which one you want to use. Must be in {0,1,2,3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032bf98f-ef15-4ec8-a124-6cb9b244c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host/Port open and accessable\n",
      "Current state: [0. 0. 0. 0.], Reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Create environment and initialize it\n",
    "from Reward.TrainingReward import TrainingReward\n",
    "env      = RealNetworkSync(action_dim=action_dim,state_dim=state_dim,circuit_id=circuit_id)\n",
    "state, _ = env.reset()\n",
    "env.render() # This function gives you the current state + reward, which both is 0 after initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac27701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mab_simulation(env, strategies, train_iters=21600, eval_iters=7200):\n",
    "    \"\"\"Run complete MAB simulation with multiple strategies\"\"\"\n",
    "    \n",
    "    # Initialize agents and storage\n",
    "    agents = {s: MABAgent(strategy=s, n_actions=125, alpha=0.1) for s in strategies}\n",
    "    results = {\n",
    "        'train': {s: [] for s in strategies},\n",
    "        'eval': {s: [] for s in strategies}\n",
    "    }\n",
    "\n",
    "    # Run training and evaluation for each strategy\n",
    "    for strategy in strategies:\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        agent = agents[strategy]\n",
    "        \n",
    "        # Training phase\n",
    "        for _ in tqdm(range(train_iters), desc=f\"Training {strategy}\"):\n",
    "            action_idx = agent.select_action()\n",
    "            _, reward, _, _, _ = env.step(agent.action_map(action_idx))\n",
    "            agent.update(action_idx, reward)\n",
    "            results['train'][strategy].append(reward)\n",
    "\n",
    "        # Evaluation phase (no updates)\n",
    "        for _ in tqdm(range(eval_iters), desc=f\"Evaluating {strategy}\"):\n",
    "            action_idx = agent.select_action() \n",
    "            _, reward, _, _, _ = env.step(agent.action_map(action_idx))\n",
    "            results['eval'][strategy].append(reward)\n",
    "\n",
    "    return pd.DataFrame(results['train']), pd.DataFrame(results['eval'])\n",
    "\n",
    "def pad_lists(d):\n",
    "    max_len = max(len(lst) for lst in d.values())\n",
    "    return {k: lst + [np.nan]*(max_len - len(lst)) for k, lst in d.items()}\n",
    "\n",
    "def run_mab_simulation_full(env, strategies, update_interval=1800):\n",
    "    \"\"\"Run MAB simulation with performance updates and stim_id handling\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'train': {s: [] for s in strategies},\n",
    "        'eval': {s: [] for s in strategies}\n",
    "    }\n",
    "    \n",
    "    total_steps = 21600 + 7200  # Total steps per strategy\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        agent = MABAgent(strategy=strategy, n_actions=25, alpha=0.1)\n",
    "        state, info = env.reset()\n",
    "        stim_id = 1 # dummy value\n",
    "        \n",
    "        with tqdm(total=total_steps, desc=f\"Running {strategy}\") as pbar:\n",
    "            while stim_id > 0:\n",
    "                action_idx = agent.select_action()\n",
    "                action = agent.action_map(action_idx)\n",
    "                \n",
    "                state, reward, terminated, truncated, info = env.step(action)\n",
    "                stim_id = info['stim_id']\n",
    "\n",
    "                if stim_id == 0:\n",
    "                    tqdm.write(\"NETWORK RESET\")\n",
    "                    break\n",
    "\n",
    "                # Phase determination and logging\n",
    "                phase = 'train' if stim_id < 21600 else 'eval'\n",
    "                results[phase][strategy].append(reward)\n",
    "                \n",
    "                # Agent updates only during training phase\n",
    "                if phase == 'train':\n",
    "                    agent.update(action_idx, reward)\n",
    "                \n",
    "                # Periodic performance updates and saving\n",
    "                if stim_id % update_interval == 0:\n",
    "                    try:\n",
    "                        window = results[phase][strategy][-update_interval:]\n",
    "                    \n",
    "                        # Calculate metrics\n",
    "                        avg_reward = np.mean(window) if window else 0\n",
    "                        cum_reward = np.sum(window) if window else 0\n",
    "                        \n",
    "                        # Build parameter string\n",
    "                        params = []\n",
    "                        if agent.strategy in ['epsilon_greedy', 'hybrid']:\n",
    "                            params.append(f\"ε={agent.epsilon:.3f}\")\n",
    "                        if agent.strategy in ['boltzmann', 'hybrid']:\n",
    "                            params.append(f\"T={agent.temperature:.3f}\")\n",
    "                        if agent.strategy == 'ucb':\n",
    "                            params.append(f\"β={agent.ucb_beta:.1f}\")\n",
    "                        if agent.strategy == 'thompson':\n",
    "                            explored = np.sum(agent.action_counts > 0)\n",
    "                            params.append(f\"Explored {explored}/{agent.n_actions}\")\n",
    "                        \n",
    "                        # Construct update message\n",
    "                        update_msg = (\n",
    "                            f\"{strategy} @ {stim_id} ({phase.upper()}): \"\n",
    "                            f\"Avg={avg_reward:.2f} | Cum={cum_reward:.0f} | \"\n",
    "                            f\"{', '.join(params)}\"\n",
    "                        )\n",
    "                        tqdm.write(update_msg)\n",
    "                        \n",
    "                        # Save results to CSV files periodically\n",
    "                        train_df = pd.DataFrame(pad_lists(results['train']))\n",
    "                        eval_df = pd.DataFrame(pad_lists(results['eval']))\n",
    "                        train_df.to_csv(f\"results/train_{strategy}.csv\")\n",
    "                        eval_df.to_csv(f\"results/eval_{strategy}.csv\")\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        continue\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "    return pd.DataFrame(pad_lists(results['train'])), pd.DataFrame(pad_lists(results['eval']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185cd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running epsilon_greedy:   4%|▍         | 1252/28800 [05:12<2:05:25,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon_greedy @ 5400 (TRAIN): Avg=0.92 | Cum=1148 | ε=0.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running epsilon_greedy:  11%|█         | 3052/28800 [12:43<1:49:04,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon_greedy @ 7200 (TRAIN): Avg=3.00 | Cum=5400 | ε=0.280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running epsilon_greedy:  13%|█▎        | 3636/28800 [15:09<1:44:49,  4.00it/s]"
     ]
    }
   ],
   "source": [
    "train_df, eval_df = run_mab_simulation_full(env, MAB_STRATEGIES)\n",
    "train_df.to_csv('full_train_results.csv', index=False)\n",
    "eval_df.to_csv('full_eval_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = run_mab_simulation(env, MAB_STRATEGIES, train_iters=1_000, eval_iters=100)\n",
    "train_df.to_csv('train_results.csv', index=False)\n",
    "eval_df.to_csv('eval_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286488a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(train_df, eval_df):\n",
    "    \"\"\"Generate comprehensive analysis of simulation results\"\"\"\n",
    "    analysis = {\n",
    "        'training': train_df.describe().T,\n",
    "        'evaluation': eval_df.describe().T,\n",
    "        'cumulative_rewards': {\n",
    "            'train': train_df.cumsum(),\n",
    "            'eval': eval_df.cumsum()\n",
    "        }\n",
    "    }\n",
    "    return analysis\n",
    "\n",
    "results_analysis = analyze_results(train_df, eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_rewards(results_analysis):\n",
    "    \"\"\"Plot training and evaluation rewards with clear phase separation\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Extract cumulative rewards from analysis\n",
    "    train_cum = results_analysis['cumulative_rewards']['train']\n",
    "    eval_cum = results_analysis['cumulative_rewards']['eval']\n",
    "\n",
    "    # Plot training curves\n",
    "    for strategy in train_cum.columns:\n",
    "        plt.plot(\n",
    "            train_cum.index, \n",
    "            train_cum[strategy], \n",
    "            label=f'{strategy} (train)',\n",
    "            alpha=0.6,\n",
    "            linewidth=1.5\n",
    "        )\n",
    "\n",
    "    # Plot evaluation curves\n",
    "    eval_start = train_cum.index[-1] + 1\n",
    "    for strategy in eval_cum.columns:\n",
    "        plt.plot(\n",
    "            eval_cum.index + eval_start,\n",
    "            eval_cum[strategy],\n",
    "            label=f'{strategy} (eval)',\n",
    "            linestyle='--',\n",
    "            linewidth=2.5\n",
    "        )\n",
    "\n",
    "    # Formatting\n",
    "    plt.axvline(eval_start, color='gray', linestyle=':', label='Train/Eval Boundary')\n",
    "    plt.fill_betweenx(\n",
    "        y=[train_cum.min().min(), train_cum.max().max()],\n",
    "        x1=eval_start,\n",
    "        x2=eval_start + len(eval_cum),\n",
    "        color='gray',\n",
    "        alpha=0.1\n",
    "    )\n",
    "    \n",
    "    plt.title('Strategy Performance Across Training and Evaluation Phases')\n",
    "    plt.xlabel('Simulation Step (stim_id)')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage with your existing analysis object\n",
    "plot_cumulative_rewards(results_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example initialization of a MAB agent\n",
    "agent = MABAgent(\n",
    "    epsilon=0.9,            # Initial exploration rate (90% random actions)\n",
    "    alpha=0.1,              # Learning rate (constant step-size)\n",
    "    initial_q=0.0,          # Optimistic initial values\n",
    "    n_actions=125,           # Number of actions, should be a power of 5\n",
    ")\n",
    "\n",
    "# Example code, that stimulates the network 100 times with a randomly sampled action, while calculating also the average reward received\n",
    "\n",
    "total_reward = 0\n",
    "action_count = 0\n",
    "\n",
    "rewards_over_time = []\n",
    "\n",
    "for _ in range(100):\n",
    "    action_idx = agent.select_action()\n",
    "    action = agent.action_map(action_idx)\n",
    "    print(f\"Stimulate with action: {action}\")\n",
    "    \n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    action_count += 1\n",
    "\n",
    "    rewards_over_time.append(reward)\n",
    "\n",
    "    agent.update(action_idx, reward)\n",
    "\n",
    "    # Plot information\n",
    "    print(f\"Info: {info}\")\n",
    "    print(f\"State: {state}, Reward: {reward}\")\n",
    "\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05caa7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
